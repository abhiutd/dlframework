name: LocationNet # name of your model
framework:
  name: MXNet # framework for the model
  version: ~0.1 # framework version contraint
version: 1.0 # version information in semantic version format
container: # containers used to perform model prediction
           # multiple platforms can be specified
  amd64:
    gpu: raiproject/carml-mxnet:amd64-cpu
    cpu: raiproject/carml-mxnet:amd64-gpu
  ppc64le:
    cpu: raiproject/carml-mxnet:ppc64le-gpu
    gpu: raiproject/carml-mxnet:ppc64le-gpu
description: >
  A photo-localization convolutional neural network based on the Inception architecture.
  LocationNet is able to localize 3.6% of images at street level, and 48% at continent level.
  The network was trained on 19M EXIF geotagged photos and consists of 97,321,048 parameters.
references: # references to papers / websites / etc.. describing the model
  - https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45488.pdf
# license of the model
license: MIT
# inputs to the model
inputs:
  # first input type for the model
  - type: image
    # description of the first input
    description: the input image
    parameters: # type parameters
      dimensions: [1, 3, 224, 224]
      mean: [123.68, 116.779, 103.939]
output:
  # the type of the output
  type: coordinates
  # a description of the output parameter
  description: the output coordinates
  parameters:
    # type parameters
    features_url: https://raw.githubusercontent.com/multimedia-berkeley/tutorials/master/grids.txt
model: # specifies model graph and weights resources
  base_url: https://s3.amazonaws.com/mmcommons-tutorial/models/
  graph_path: RN101-5k500-symbol.json
  weights_path: RN101-5k500-0012.params
  is_archive: false # if set, then the base_url is a url to an archive
                    # the graph_path and weights_path then denote the
                    # file names of the graph and weights within the archive
attributes: # extra network attributes
  kind: CNN # the kind of neural network (CNN, RNN, ...)
  training_dataset: MultimediaCommons # dataset used to for training
  manifest_author: abduld
